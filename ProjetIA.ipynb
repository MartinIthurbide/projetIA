{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986e691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pa\n",
    "letters_train = pa.read_csv(\n",
    "    'https://www.labri.fr/perso/zemmari/datasets/emnist/emnist-letters-train.csv',\n",
    "    header=None)\n",
    "letters_test = pa.read_csv(\n",
    "    'https://www.labri.fr/perso/zemmari/datasets/emnist/emnist-letters-test.csv',\n",
    "    header=None)\n",
    "def load_letters():\n",
    "    X_train = np.array(letters_train.iloc[:, 1:785])\n",
    "    y_train = np.array(letters_train.iloc[:, 0])\n",
    "    y_train = y_train - 1\n",
    "    X_test = np.array(letters_test.iloc[:, 1:785])\n",
    "    y_test = np.array(letters_test.iloc[:, 0])\n",
    "    y_test = y_test -1 \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 28, 28))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], 28, 28))\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd7d8b8",
   "metadata": {},
   "source": [
    "# Partie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a8227f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "X_train shape: (88800, 28, 28, 1)\n",
      "y_train shape: (88800, 26)\n",
      "X_test shape: (14800, 28, 28, 1)\n",
      "y_test shape: (14800, 26)\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "(X_train, y_train), (X_test, y_test) = load_letters()\n",
    "img_rows, img_cols = X_train.shape[1], X_train.shape[2]\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train  = X_train / 255\n",
    "X_test  = X_test / 255\n",
    "\n",
    "unique_classes = np.unique(y_train)\n",
    "print(unique_classes)\n",
    "\n",
    "num_classes = unique_classes.size\n",
    "\n",
    "\n",
    "y_train = to_categorical(y_train,num_classes)\n",
    "y_test = to_categorical(y_test,num_classes)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9747bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               204928    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 26)                3354      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 227,098\n",
      "Trainable params: 227,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "888/888 [==============================] - 48s 51ms/step - loss: 0.5878 - accuracy: 0.8185 - val_loss: 0.3882 - val_accuracy: 0.8723\n",
      "Epoch 2/10\n",
      "888/888 [==============================] - 46s 52ms/step - loss: 0.2767 - accuracy: 0.9096 - val_loss: 0.3079 - val_accuracy: 0.8961\n",
      "Epoch 3/10\n",
      "888/888 [==============================] - 42s 47ms/step - loss: 0.2212 - accuracy: 0.9260 - val_loss: 0.2663 - val_accuracy: 0.9091\n",
      "Epoch 4/10\n",
      "888/888 [==============================] - 45s 51ms/step - loss: 0.1917 - accuracy: 0.9345 - val_loss: 0.2474 - val_accuracy: 0.9163\n",
      "Epoch 5/10\n",
      "888/888 [==============================] - 47s 52ms/step - loss: 0.1672 - accuracy: 0.9417 - val_loss: 0.2415 - val_accuracy: 0.9164\n",
      "Epoch 6/10\n",
      "888/888 [==============================] - 48s 54ms/step - loss: 0.1484 - accuracy: 0.9479 - val_loss: 0.2419 - val_accuracy: 0.9207\n",
      "Epoch 7/10\n",
      "888/888 [==============================] - 49s 55ms/step - loss: 0.1335 - accuracy: 0.9515 - val_loss: 0.2565 - val_accuracy: 0.9166\n",
      "Epoch 8/10\n",
      "888/888 [==============================] - 48s 54ms/step - loss: 0.1187 - accuracy: 0.9558 - val_loss: 0.2570 - val_accuracy: 0.9195\n",
      "Epoch 9/10\n",
      "888/888 [==============================] - 50s 56ms/step - loss: 0.1060 - accuracy: 0.9589 - val_loss: 0.2616 - val_accuracy: 0.9176\n",
      "Epoch 10/10\n",
      "888/888 [==============================] - 47s 53ms/step - loss: 0.0963 - accuracy: 0.9624 - val_loss: 0.2562 - val_accuracy: 0.9213\n",
      "463/463 [==============================] - 3s 6ms/step - loss: 0.2562 - accuracy: 0.9213\n",
      "Test loss: 0.2562299370765686\n",
      "Test accuracy: 0.9212837815284729\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "def letter_network():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(26, activation='softmax')) #26 lettres dans l'alphabet\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = letter_network()\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=100, validation_data=(X_test, y_test))\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"Test loss:\", scores[0])\n",
    "print(\"Test accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a390205",
   "metadata": {},
   "source": [
    "# Partie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb9aaf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (148800, 28, 28, 1)\n",
      "y_train shape: (148800, 26)\n",
      "X_test shape: (24800, 28, 28, 1)\n",
      "y_test shape: (24800, 19)\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "from keras.datasets import mnist\n",
    "# load des lettres\n",
    "(X_train_letters, y_train_letters), (X_test_letters, y_test_letters) = load_letters()\n",
    "\n",
    "# load des chiffres\n",
    "(X_train_numbers, y_train_numbers), (X_test_numbers, y_test_numbers) = mnist.load_data()\n",
    "\n",
    "# on concatene les donnÃ©es\n",
    "X_train = np.concatenate((X_train_letters,X_train_numbers))\n",
    "y_train = np.concatenate((y_train_letters,y_train_numbers))\n",
    "\n",
    "X_test = np.concatenate((X_test_letters,X_test_numbers))\n",
    "y_test = np.concatenate((y_test_letters,y_test_numbers))\n",
    "\n",
    "\n",
    "# on definit la taille\n",
    "img_rows, img_cols = X_train.shape[1], X_train.shape[2]\n",
    "\n",
    "# On reshape en 1D\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# on transforme y_train et y_test en vecteurs\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ac2cbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               204928    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 224,002\n",
      "Trainable params: 224,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (100, 26) and (100, 2) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m neural_network()\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, scores[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileeviipaga.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (100, 26) and (100, 2) are incompatible\n"
     ]
    }
   ],
   "source": [
    "def neural_network():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Conv2D(64,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = neural_network()\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=100, validation_data=(X_test, y_test))\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"Test loss:\", scores[0])\n",
    "print(\"Test accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30ff36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
